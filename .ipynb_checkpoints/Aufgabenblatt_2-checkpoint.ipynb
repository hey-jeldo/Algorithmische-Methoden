{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import matplotlib\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1\n",
    "Wir bauen uns ein Korpus . . .\n",
    "- Wählen Sie mindestens 4 Online-Artikel aus, die aus unterschiedlichen Zei-\n",
    "tungsportalen stammen oder unterschiedlichen Genres zuzurechnen sind.\n",
    "Speichern Sie diese Artikel anschließend als reine Textdateien (Kodierung: UTF-8).\n",
    "- Erzeugen Sie ein Korpus, das diese Texte enthält (PlaintextCorpusReader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbeitsverzeichnis laden und in den  Unterordner '/data' gehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = os.getcwd() + \"/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle txt-Dateien (ausser Stoppworte.txt) in '/data' matchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_texts = \".+\\.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texte als Korpus der Variable 'mein_korpus' zuweisen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mein_korpus = PlaintextCorpusReader(corpus_dir, corpus_texts, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Überprüfen ob Korpusbildung erfolgreich war:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bushido.txt', 'corbyn.txt', 'corona.txt', 'stoppworte.txt', 'trump.txt']\n"
     ]
    }
   ],
   "source": [
    "print(mein_korpus.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2.1\n",
    "Berechnen Sie die Länge in Token und den Vokabularumfang (Types)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Texte in ein Dictionary ablegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = {\n",
    "    \"Trump\" : mein_korpus.words('trump.txt'),\n",
    "    \"Bushido\" : mein_korpus.words('bushido.txt'),\n",
    "    \"Corbyn\" : mein_korpus.words('corbyn.txt'),\n",
    "    \"Corona\" : mein_korpus.words('corona.txt')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable 'all' für alle Texte im Korpus anlegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = corpus_dict[\"Trump\"] + corpus_dict[\"Bushido\"] + corpus_dict[\"Corbyn\"] + corpus_dict[\"Corona\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den gesamten Korpus als Schlüssel ablegen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict.update({'Korpus' : all})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Länge und Umfang der Texte drucken in dem ich durch das Dictionary iteriere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Trump) \tLänge: 1285 \tUmfang: 545\n",
      "(Bushido) \tLänge: 617 \tUmfang: 301\n",
      "(Corbyn) \tLänge: 427 \tUmfang: 249\n",
      "(Corona) \tLänge: 820 \tUmfang: 435\n",
      "(Korpus) \tLänge: 3149 \tUmfang: 1187\n"
     ]
    }
   ],
   "source": [
    "for text in corpus_dict:\n",
    "    print('(' + text + ')', '\\tLänge:', len(corpus_dict[text]), '\\tUmfang:', len(set(corpus_dict[text])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2.2\n",
    "Erstellen Sie eine Frequenzliste und lassen Sie sich die 10 häufigsten Wörter anzeigen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inkl. Stopwörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump \n",
      "\n",
      "[(',', 74), ('.', 59), ('der', 31), ('die', 26), ('Trump', 24), ('in', 21), ('Biden', 21), ('-', 20), ('sich', 16), ('und', 16)] \n",
      "\n",
      "Bushido \n",
      "\n",
      "[('.', 30), (',', 27), ('Bushido', 15), ('-', 14), ('der', 14), ('Abou', 13), ('Chaker', 11), ('habe', 11), ('und', 10), ('„', 8)] \n",
      "\n",
      "Corbyn \n",
      "\n",
      "[(',', 19), ('.', 19), ('der', 15), ('die', 10), ('in', 8), ('-', 7), ('und', 7), ('Corbyn', 6), ('Labour', 6), ('„', 6)] \n",
      "\n",
      "Corona \n",
      "\n",
      "[(',', 59), ('.', 36), ('die', 22), ('der', 15), ('in', 15), ('-', 14), ('?', 11), (':', 10), ('wird', 9), ('werden', 8)] \n",
      "\n",
      "Korpus \n",
      "\n",
      "[(',', 179), ('.', 144), ('der', 75), ('die', 63), ('-', 55), ('in', 52), ('und', 41), ('zu', 31), (':', 31), ('sich', 29)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in corpus_dict:\n",
    "    print(text, '\\n')\n",
    "    fd = nltk.FreqDist(corpus_dict[text])\n",
    "    print(fd.most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ohne Stoppwörter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stoppwörter aus ihrer txt-Datei in Variable lesen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppworte = mein_korpus.words('stoppworte.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump\n",
      " [('Trump', 24), ('Biden', 21), ('nicht', 11), ('Duell', 7), ('schon', 7), ('Bidens', 7), ('Hunter', 7), ('Trumps', 6), ('Sohn', 6), ('Wähler', 5)] \n",
      "\n",
      "Bushido\n",
      " [('Bushido', 15), ('Abou', 13), ('Chaker', 11), ('soll', 8), ('Treffen', 5), ('2017', 4), ('Mittwoch', 4), ('Rapper', 3), ('Arafat', 3), ('wollte', 3)] \n",
      "\n",
      "Corbyn\n",
      " [('Corbyn', 6), ('Labour', 6), ('Starmer', 5), ('Jeremy', 4), ('nicht', 4), ('Keir', 4), ('EHRC', 4), ('Antisemitismus', 4), ('Partei', 3), ('Suspendierung', 3)] \n",
      "\n",
      "Corona\n",
      " [('nicht', 6), ('Impfstoffe', 5), ('bald', 4), ('erste', 4), ('Impfstoff', 4), ('Kann', 3), ('SPIEGEL', 3), ('Sahin', 3), ('schützen', 3), ('Wochen', 3)] \n",
      "\n",
      "Korpus\n",
      " [('Trump', 24), ('nicht', 24), ('Biden', 21), ('Bushido', 15), ('Abou', 13), ('Chaker', 11), ('schon', 10), ('noch', 9), ('soll', 9), ('Duell', 7)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in corpus_dict:\n",
    "    wortliste = list(corpus_dict[text])\n",
    "    wortliste = [wort for wort in wortliste if len(wort) > 3]\n",
    "    wortliste = [wort for wort in wortliste if wort not in stoppworte]\n",
    "    fd = nltk.FreqDist(wortliste)\n",
    "    print(text + '\\n', fd.most_common(10), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
