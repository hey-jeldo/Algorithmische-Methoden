---
title: "Lösungen aus *Algorithmische Methoden*"
runningheader: "Bearbeitung der Aufgabenblätter aus der Vorlesung" # only for pdf output
subtitle: "Bearbeitung der Aufgabenblätter aus der Vorlesung" # only for html output
author: "Jeldo Arno Meppen"
date: "`r Sys.Date()`"
monofont: Hack
output:
  tufte::tufte_html:
    highlight: kate
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
  beamer: default
---

```{r setup, include=FALSE}
library(tufte)
library(reticulate)
# use_python("/Users/jeldo/opt/anaconda3/bin/python")
# invalidate cache when the tufte version changes
py_install("nltk")
py_install("matplotlib")
knitr::opts_chunk$set(cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

# Vorbereitungen

Um alle uns gestellten Aufgaben lösen zu können, bedarf es des `import`s verschiedener Python-Bibliotheken oder Module. Für uns besonders wichtig sind `re` für *reguläre Ausdrücke*, `nltk` für *Verarbeitung natürlicher Sprache* und `matplotlib` für das Erstellen mathematischer Darstellungen wie *Graphen*.

```{python}
import os
import re
import nltk
from nltk.corpus import PlaintextCorpusReader
import matplotlib
```

# Aufgabenblatt 1

## Aufgabe 1.1

> Weisen Sie einer Variable die Zeichenkette `”Trier, den 3.11.2020\nMit freundliche Grüßen”` zu und geben Sie einen regulären Ausdruck an, der das vollständige Datum extrahiert.
Können Sie mit dem von Ihnen gewählten regulären Ausdruck jeweils auch die Tages-, Monats- und Jahresangabe extrahieren?

```{python}
ort_datum = "Trier, den 3.11.2020\nMit freundliche Grüßen"
result = re.search("((\d+)[./-](\d+)[./-](\d+))", ort_datum)
# [./-] hilft bei MM/DD/YY oder YYYY-MM-DD
date = result.group(1)
day = result.group(2)
month = result.group(3)
year = result.group(4)
print(f"Heute ist der {date}, oder ISO: {year}-{month}-{day}")
# gilt nur für DDMMYY(YY)
```

## Aufgabe 1.2

> Weisen Sie einer Variable die Zeichenkette `"Betty bought a bit of butter, But the butter was so bitter, So she bought some better butter, To make the bitter butter better."` zu und geben Sie einen regulären Ausdruck an, der alle Wörter extrahiert, die mit `"b"` oder `"B"` beginnen.

```{python}
rhyme = '''
Betty bought a bit of butter, But the butter was so bitter, 
So she bought some better butter, To make the bitter butter better.'''
print(re.findall("[Bb]\w+", rhyme))
```

## Aufgabe 1.3

> Geben Sie einen regulären Ausdruck an, der aus einem Text, der eine Email-Adresse enhält diese Email-Adresse extrahiert.
Achten Sie darauf, dass Sie auch auf die user-id, die Domäne und den Suffix zugreifen können.

```{python}
emails = "test.acc+uni@g-mail.co.jp, notme@hey.com"
result = re.findall("(([\w\.\-]+)\+?[\w]*@([\w\-]+)(\.[\w]+\.?[\w]*))", emails)
for mail in result:
  print(mail)
```

## Aufgabe 1.4

> Gegeben sei eine Zeichenkette, die unnötige Leerzeichen, Tabs und überflüssige Interpunktionszeichen enthält.
Geben Sie einen Ausdruck an, der diese Zeichen entfernt.

```{python}
text = "     Dieser,     Anblick   spottet;    jeder  Beschreibung,!   "
result = re.findall("([^\s,;]+)", text)
print(*result)
```

# Aufgabenblatt 2

## Aufgabe 2.1

1. Wählen Sie mindestens 4 Online-Artikel aus, die aus unterschiedlichen Zeitungsportalen stammen oder unterschiedlichen Genres zuzurechnen sind.
2. Speichern Sie diese Artikel anschließend als reine Textdateien (Kodierung: UTF-8).
3. Erzeugen Sie ein Korpus, das diese Texte enthält (PlaintextCorpusReader).

```{python}
corpus_dir = os.getcwd() + "/data"
corpus_texts = ".+\.txt"
mein_korpus = PlaintextCorpusReader(corpus_dir, corpus_texts, encoding='utf-8')
print(mein_korpus.fileids())
```

## Aufgabe 2.2.1

> Berechnen Sie die Länge in Token und den Vokabularumfang (Types).

```{python}
corpus_dict = {
    "Trump" : mein_korpus.words('trump.txt'),
    "Rapper" : mein_korpus.words('bushido.txt'),
    "Corbyn" : mein_korpus.words('corbyn.txt'),
    "Corona" : mein_korpus.words('corona.txt')
}
all = corpus_dict["Trump"]
all += corpus_dict["Rapper"]
all += corpus_dict["Corbyn"]
all += corpus_dict["Corona"]
corpus_dict.update({'Korpus' : all})
for text in corpus_dict:
    print(text + ':', '\tLänge:', len(corpus_dict[text]), '\tUmfang:', len(set(corpus_dict[text])))
```

```{python}
for text in corpus_dict:
    fd = nltk.FreqDist(corpus_dict[text])
    print(text + ':', fd.most_common(10), '\n')
```

```{python}
for text in corpus_dict:
    fd = nltk.FreqDist(corpus_dict[text])
    print(text + ':')
    fd.plot(10, cumulative=True)
```

# Arbeitsblatt 3

## Aufgabe 3.1

## Aufgabe 3.2

## Aufgabe 3.3


---

# Arbeitsblatt 4

## Aufgabe 4.1

> Definieren Sie eine Funktion `stamm_erweiterung`, die einen Wortstamm $w$ und eine Liste $L$ von Wörtern/Token als Argumente nimmt und als Wert eine Liste von Wörtern aus $L$ liefert, die den Stamm $w$ enthalten.

```{python}
def stamm_erweiterung(wortstamm, liste):
    ergebnis = []
    for wort in liste:
        if wortstamm in wort:
            ergebnis.append(wort)
    return ergebnis
```

```{python}
stamm_erweiterung("impf", mein_korpus.words('corona.txt'))
```
## Aufgabe 4.2

> Definieren Sie eine Funktion sprach_identifikation, die einen Text bzw. Korpus
als Argument nimmt und feststellt, in welcher Sprache er geschrieben ist.
Die Funktion soll dazu die in NLTK für verschiedene Sprachen definierten Stop- wortlisten verwenden und als Wert die Sprache ausgeben, für die der Text die größte Trefferzahl (enthaltene Stopwörter) aufweist.

```{python}
from nltk.corpus import stopwords
```

```{python}
def sprach_vergleich(text, sprache):
    stopwort_treffer = []
    for wort in text:
        if wort in stopwords.words(sprache):
            stopwort_treffer.append(wort)
    return len(stopwort_treffer)
```

```{python}
def sprach_identifikation(text):
    treffermenge = {}
    for sprache in stopwords.fileids():
        länge = sprach_vergleich(text, sprache)
        treffermenge[sprache] = länge
    sortiert = sorted(treffermenge, key=treffermenge.get)
    return sortiert[-1]
```

```{python}
sprach_identifikation(mein_korpus.words('biden-iraq.txt'))
sprach_identifikation(mein_korpus.words('corona.txt'))
sprach_identifikation(mein_korpus.words('turk.txt'))
```

